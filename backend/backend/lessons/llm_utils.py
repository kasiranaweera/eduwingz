"""
LLM Utilities for lesson generation
Provides a simple interface to use Qwen LLM for generating educational content
"""

import os
import sys
import json
import traceback
from typing import List, Dict, Optional

# Add FastAPI app to path
fastapi_path = os.path.join(os.path.dirname(__file__), '../../fastapi_app')
if fastapi_path not in sys.path:
    sys.path.insert(0, fastapi_path)


class LessonTopicGenerator:
    """Generate lesson topics using Qwen LLM"""
    
    _llm_client = None
    _initialized = False
    
    @classmethod
    def _init_llm(cls):
        """Initialize LLM client"""
        if cls._initialized:
            return cls._llm_client
        
        try:
            from llm_model import LLM_Model
            from langchain_core.messages import SystemMessage, HumanMessage
            
            llm_model = LLM_Model()
            cls._llm_client = llm_model.get_client()
            cls._initialized = True
            print("âœ… LLM client initialized successfully")
            return cls._llm_client
            
        except ImportError as e:
            print(f"âš ï¸ Could not import LLM from FastAPI app: {e}")
            return None
        except Exception as e:
            print(f"âš ï¸ Error initializing LLM: {e}")
            return None
    
    @classmethod
    def generate_topics(cls, grade: str, subject: str, topic: str, attachments: Optional[List[Dict]] = None) -> List[Dict]:
        """
        Generate lesson topics using Qwen LLM
        
        Args:
            grade: Grade level
            subject: Subject name
            topic: Topic name
            attachments: Optional list of educational material attachments
            
        Returns:
            List of generated topics with title, content, and order
        """
        try:
            # Get LLM client
            llm_client = cls._init_llm()
            if not llm_client:
                raise RuntimeError("Failed to initialize LLM client")
            
            from langchain_core.messages import SystemMessage, HumanMessage
            
            # Build attachment context
            attachment_context = ""
            if attachments:
                attachment_context = "\n\nAvailable educational materials:\n"
                for attachment in attachments:
                    attachment_context += f"- {attachment.get('name', attachment.get('type'))}\n"
            
            # Create system and user prompts
            system_prompt = """You are an expert educational content creator. Generate comprehensive, well-structured learning topics for students.

Your response MUST be in valid JSON format ONLY:
{
    "topics": [
        {
            "title": "Topic Title",
            "content": "Comprehensive content for this topic...",
            "order": 1
        }
    ]
}

Create 4 main topics that progressively build understanding."""

            user_message = f"""Create 4 comprehensive learning topics in JSON format for:
- Grade Level: {grade}
- Subject: {subject}
- Main Topic: {topic}
{attachment_context}

Progression should be:
1. Introduction - Basic concepts and overview
2. Core Concepts - Detailed theory and principles
3. Practical Applications - Real-world examples and case studies
4. Summary - Key takeaways and conclusions

Content must be appropriate for Grade {grade} students.

Return ONLY the JSON object, no markdown or extra text."""

            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_message)
            ]
            
            print(f"ðŸ¤– Generating topics for '{topic}' in {subject} (Grade {grade}) using Qwen LLM...")
            
            # Call LLM
            response = llm_client.invoke(messages)
            response_text = response.content.strip() if hasattr(response, 'content') else str(response).strip()
            
            print(f"ðŸ“ LLM response length: {len(response_text)} chars")
            
            # Parse JSON response
            topics = cls._parse_json_response(response_text)
            
            if not topics:
                raise ValueError("No topics generated by LLM")
            
            print(f"âœ… Successfully generated {len(topics)} topics")
            return topics
            
        except Exception as e:
            print(f"âŒ Error in generate_topics: {str(e)}")
            traceback.print_exc()
            raise
    
    @staticmethod
    def _parse_json_response(response_text: str) -> List[Dict]:
        """
        Parse JSON from LLM response
        
        Handles:
        - Direct JSON
        - JSON in markdown code blocks (```json ... ```)
        - JSON in generic code blocks (``` ... ```)
        """
        try:
            # Try direct JSON parsing
            data = json.loads(response_text)
            return data.get('topics', [])
            
        except json.JSONDecodeError:
            pass
        
        # Try markdown code blocks
        if "```json" in response_text:
            try:
                start = response_text.find("```json") + 7
                end = response_text.find("```", start)
                json_str = response_text[start:end].strip()
                data = json.loads(json_str)
                return data.get('topics', [])
            except:
                pass
        
        # Try generic code blocks
        if "```" in response_text:
            try:
                start = response_text.find("```") + 3
                end = response_text.find("```", start)
                json_str = response_text[start:end].strip()
                data = json.loads(json_str)
                return data.get('topics', [])
            except:
                pass
        
        # Last resort: try to find JSON object in response
        try:
            start = response_text.find('{')
            end = response_text.rfind('}') + 1
            if start != -1 and end > start:
                json_str = response_text[start:end]
                data = json.loads(json_str)
                return data.get('topics', [])
        except:
            pass
        
        raise ValueError(f"Could not parse JSON from LLM response:\n{response_text[:500]}")
